---
layout: post
title:  "Week 2"
date:   2014-09-22 09:22:44
categories: bikeshare
---
## First Look ##
### Preprocess the data ###
* The dataset has `10886 observations`, which are made on the `first 19 days `of each month in 2011 and 2012.
* The dataset has `12 varibles`
  * Explanatory variable
    * Factor: `weather`, `season`,`holiday`,`workingday`
    * Numeric: `temp`,`atemp`,`humidity`,`windspeed`
    * timestamp:`datetime`
  * Response variable
    * `count`
    * `registered` + `casual` = `count`


### Add `hour` variable ###
According to common sense, time affects the count of rides. And we can see this trend in the following plot. For this reason, I created a `factor` variable `hour` from `datetime`.
![hour_count_boxplot](https://docs.google.com/uc?export=view&id=0B47woKFE0zXeX1hMM0w0Rml1dG8)


### Explore `count` ###
#### count, casual and registered
![count](https://docs.google.com/uc?export=view&id=0B47woKFE0zXeLUszYkFpaGM4WGc)

```
    count         registered     casual
  Min.   :  1.0  Min.   :  0.0   Min.   :  0.00
  1st Qu.: 42.0  1st Qu.: 36.0   1st Qu.:  4.00
  Median :145.0  Median :118.0   Median : 17.00
  Mean   :191.6  Mean   :155.6   Mean   : 36.02
  3rd Qu.:284.0  3rd Qu.:222.0   3rd Qu.: 49.00
  Max.   :977.0  Max.   :886.0   Max.   :367.00  
```

* count,registered and casual have similar distribution (right skewed)
* `casual` is generally smaller than `registered`

#### `count` vs factors
![count and categorial variable](https://docs.google.com/uc?export=view&id=0B47woKFE0zXeYUt3R0hIVE55Rk0)

* `weather` : The better the weather is, the larger the count is.
* `season` : We see more rides in `summer` and `fall` than `spring` and `winter`.
* `holiday` and `workingday`: There is no obvious effect on count.
* We can see lots of outliers in the boxplot. This is reasonable as the data size is large.

#### `count` vs numeric
![count and numerical variable](https://docs.google.com/uc?export=view&id=0B47woKFE0zXeM3FMYlF4V2NIbkU)

* `temp` and `atemp`: Higher temperature tends to have more rides.
* `humidity` : There's no obvious effect.
* `windspeed`: Smaller windspeed tends to have more rides.


## Model 1 : Linear Regression ##
I do linear regression with all variables in the data with the following code:

```python
count.feature = train[,-c(10,11)] #remove casual and registered from varaibles
train.lm <- lm(count ~ ., data=count.feature) #linear regression
```

### Interpretation of R's output

```
Coefficients:
              Estimate Std. Error t value Pr(>|t|)
(Intercept) -3.598e+03  8.997e+01 -39.990  < 2e-16 ***
datetime     2.762e-06  6.488e-08  42.570  < 2e-16 ***
temp         2.518e+00  8.378e-01   3.005 0.002662 **
atemp        2.775e+00  7.332e-01   3.785 0.000154 ***
humidity    -8.012e-01  7.235e-02 -11.074  < 2e-16 ***
windspeed   -4.219e-01  1.388e-01  -3.040 0.002369 **
```
* All numeric variables have some effects on count.
* atemp and temp have similar positive coefficients but atemp has smaller sd.
* humidity and windspeed have negative effects on count.

```
holiday1    -5.399e+00  6.351e+00  -0.850 0.395362
workingday1  1.975e+00  2.273e+00   0.869 0.384925
```

* holiday and workingday: No significant effect.

```
season2      2.591e+01  3.808e+00   6.804 1.07e-11 ***
season3     -7.196e+00  4.920e+00  -1.463 0.143610
season4      3.838e+00  3.460e+00   1.109 0.267365
weather.L    1.464e+02  7.171e+01   2.041 0.041271 *  
weather.Q   -6.375e+01  5.346e+01  -1.193 0.233088
weather.C    8.449e+00  2.405e+01   0.351 0.725379
```
* season:  only Summer indicator has significant effect on count.
* weather: only `small rain` indicator has effect on count.

```
hour.L       1.963e+02  5.323e+00  36.882  < 2e-16 ***
hour.Q      -4.021e+02  5.660e+00 -71.047  < 2e-16 ***
hour.C      -1.348e+02  5.283e+00 -25.514  < 2e-16 ***
hour^4      -3.619e+01  5.185e+00  -6.980 3.13e-12 ***
hour^5      -1.355e+01  5.041e+00  -2.689 0.007187 **
hour^6       2.113e+02  5.038e+00  41.948  < 2e-16 ***
hour^7       1.285e+01  5.019e+00   2.561 0.010452 *  
hour^8      -1.526e+02  5.020e+00 -30.398  < 2e-16 ***
hour^9       4.264e+00  5.020e+00   0.849 0.395638
hour^10     -1.061e+00  5.020e+00  -0.211 0.832679
hour^11      4.198e+00  5.018e+00   0.837 0.402836
hour^12      1.118e+02  5.017e+00  22.276  < 2e-16 ***
hour^13     -2.259e+01  5.014e+00  -4.506 6.69e-06 ***
hour^14     -1.047e+02  5.011e+00 -20.900  < 2e-16 ***
hour^15      3.092e+01  5.006e+00   6.177 6.78e-10 ***
hour^16      2.642e+01  5.003e+00   5.281 1.31e-07 ***
hour^17     -1.223e+01  5.001e+00  -2.446 0.014448 *  
hour^18      3.627e+01  4.999e+00   7.255 4.29e-13 ***
hour^19     -6.368e+00  4.999e+00  -1.274 0.202709
hour^20     -2.705e+01  4.999e+00  -5.411 6.39e-08 ***
hour^21      2.306e+00  4.999e+00   0.461 0.644663
hour^22     -1.530e+01  4.998e+00  -3.062 0.002204 **
hour^23     -4.679e+00  4.997e+00  -0.936 0.349085
```
* `hour`: {1 - 8,12 - 18,20, 22} have effect on count.

```
Residual standard error: 106.7 on 10849 degrees of freedom
Multiple R-squared:  0.6543,  Adjusted R-squared:  0.6532
F-statistic: 570.5 on 36 and 10849 DF,  p-value: < 2.2e-16
```
* R-squared: 0.6543 (Explained variation / Total variation). I use this as an benchmark to compare different models.
* F-statistics: p-value is small and there is at least one variable has effect on count.


### Diagnosis of linear regression model ###
![Diagnosis plot of linear regression](https://docs.google.com/uc?export=view&id=0B47woKFE0zXeTVpDd1NzWk9KWVk)
* `Equal Variance` : violated.
* `Normal`: The curve in qqplot(third) is almost linear.
* `Outliers`: We see lots of outliers in the forth plot.
* `Independent`:

### Prediction ###
* The test dataset has `6863 observations`, which are made on the `last 10 days `of each month in 2011 and 2012.
* `583` predictions are smaller than 0. I set those predictions = 0 mannully.
* Kaggle's resutl: `rmsle = 1.03653` ranking 746/923.

#### Evaluation -- RMSLE ####
'Kaggle' using RMSLE to evaluate the result. The evaluation equation is :
<img src="http://bit.ly/1EwgeFb"alt=" \sqrt{ \frac{1}{n} \sum_1^n((log(p_i+1) - log(a_i+1))^2 } \\p_i \text{is the ith predicted value}\\a_i \text{is the ith actual value}" />

  Following plot showes how this works visually(suppose a = 200):
![rmsle_image](https://docs.google.com/uc?export=view&id=0B47woKFE0zXeZFFJUHFiNE9DYW8)

  We can see the farther the predicted value is from actual value, the bigger the rmsle. And the rmsle is relatively bigger when predicted value is small.

# Model 2: `Count` = `Registered` + `Casual` #
### Add `weekday` variable ###
In model 1, both `workingday` and `holiday` does not show a significant linear relationship with `count`. But it is reasonable to include `weekday` when predicting `registered` and `casual`.
![count and numerical variable](https://docs.google.com/uc?export=view&id=0B47woKFE0zXeNkt5ODlvMW5UaW8)

### Linear Regression of `casual` and `registered`
Similarly, I do linear regression on `casual` and `registered`.

```
                  registered       Casual
                  coef   sig    coef   sig
(Intercept)      -3166.33 *    -453.458 *
datetime         0        *    0        *
season2          16.561   *    9.237    *
season3          -6.282        -1.173
season4          4.848         -1.428
holiday1         10.725        -8.434   *
workingday1      37.939   *    -32.151  *
weekdayTuesday   6.203         -2.51    *
weekdayWednesday 8.455    *    -2.125   *
weekdayThursday  10.47    *    -2.368   *
weekdayFriday    7.212    *    4.967    *
weekdaySaturday  12.51    *    7.423    *
```
* Date variable have opposite effects on casual and registered.
* weekdaySunday is not included because Sunday dependes on workingday and weekSaturday.

```
weather.L        129.96   *    12.953   *
weather.Q        -58.579       -2.187
weather.C        7.539         -0.618
temp             0.818         1.468    *
atemp            2.41     *    0.6      *
humidity         -0.461   *    -0.312   *
windspeed        -0.19         -0.203   *
```
* Weather variables have similar effects on the casual and registered.
* Weather variables have slightly bigger effects on casual than registered.

```
hour.L           157.998  *    38.731   *
hour.Q           -318.345 *    -84.385  *
hour.C           -99.732  *    -35.417  *
hour^4           -68.184  *    32.265   *
hour^5           -23.522  *    10.098   *
hour^6           217.74   *    -6.485   *
hour^7           13.092   *    -0.242   *
hour^8           -152.609 *    0.001    *
hour^9           8.638    *    -4.343   *
hour^10          -0.464        -0.635
hour^11          -0.011        4.261    *
hour^12          108.647  *    3.087    *
hour^13          -20.829  *    -1.775   *
hour^14          -102.62  *    -2.097   *
hour^15          30.305   *    0.605    *
hour^16          26.324   *    0.092    *
hour^17          -13.326  *    1.082    *
hour^18          34.636   *    1.649    *
hour^19          -4.994        -1.376
hour^20          -24.965  *    -2.086   *
hour^21          3.074         -0.781
hour^22          -14.019  *    -1.282   *
hour^23          -4.917        0.23

```
* Time variables affects registered more than casual.
* `casual` and `registered` have similar significant variables.

```
Registered:
Residual standard error: 90.2 on 10844 degrees of freedom
Multiple R-squared:  0.6447,	Adjusted R-squared:  0.6434
F-statistic:   480 on 41 and 10844 DF,  p-value: < 2.2e-16
Casual:
Residual standard error: 32.4 on 10844 degrees of freedom
Multiple R-squared:  0.581,	Adjusted R-squared:  0.5794
F-statistic: 366.7 on 41 and 10844 DF,  p-value: < 2.2e-16
```
* According to R-quared, the linear model explains `registered` better.
* `rmsle.casual = 1.007805` `rmsle.count = 0.8792593`
### Combination and Prediction ###
* `count = casual + registered` to predict count in test data.
*  `rmsle` on training data is 0.85405.
* Kaggle's result: `rmsle = 0.92049`. ranking 765/982.

## Variable Selection ###
* R-square is a good indicator of model because our goal is to make accurate predictions.
* I perform all-subsets regression using the leaps( ) function from the leaps package.
![all_subsets_regression](https://docs.google.com/uc?export=view&id=0B47woKFE0zXeb0R3QzdoZTZxMUk)
* We see from the plot that the best r-square is achieved by including all variables in the training data.

### Other tricks ###
1. `Rmsle` penalizes more for smaller prediction than bigger prediction. Manually setting predictions, which are smaller than 10, to 10 improve the performance a lot.(rmsle = 0.836)
2. I also try support vector machine with linear kernel. (`rmsle = 0.732`)


### Future Improvement ###
1. linear regression(interaction, multicollinearity)
![all_subsets_regression](https://docs.google.com/uc?export=view&id=0B47woKFE0zXeQXZUMU5Xc1RIZVk)
2. Non-linear regression

####Reference and Useful Link ####
1. [Introduction to Multiple linear regression](http://dept.stat.lsa.umich.edu/~kshedden/Courses/Stat401/Notes/401-multreg.pdf)
1. [How to interpret linear regression output](http://dss.princeton.edu/online_help/analysis/interpreting_regression.htm)
2. [How to run a linear regression in R and do evaluation](http://brandonharris.io/kaggle-bike-sharing/)
3. [How to build a simple model one step by step](http://brandonharris.io/kaggle-bike-sharing/)
